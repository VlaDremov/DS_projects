{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» c BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import logging\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перебросим расчеты эмбеддингов на GPU (при наличии ядер CUDA). Тестировал на своей машине - ускорение примерно в 5 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим дизбаланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.898321\n",
      "1    0.101679\n",
      "Name: toxic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['toxic'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дизбаланс классов впечатляющий. Учитывая скорость расчета эмбеддингов, не будем делать downsampling для балансировки классов, на практике для этого датасета он показал себя хуже, чем гиперпараметр class_weight = 'balanced' у моделей.\n",
    "\n",
    "Сделаем семпл, близкий к исходному датафрейму, но содержащий кол-во элементов, кратное 50. 50 - размер батча. Кратное 50 нужно сделать, чтобы впоследствии фичи и таргеты не расходились в количестве (неполный батч эмбеддиться не будет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = np.concatenate(embeddings)\n",
    "#df_sample = df[df['text'].str.len() <= 512].sample(8000).reset_index(drop = True)\n",
    "#df_sample = df[:100000]\n",
    "df = df.sample(159500).reset_index(drop = True)\n",
    "features = df['text']\n",
    "target = df['toxic']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ускорения расчетов используем DistilBert, тем более, что обычный Bert на данном датафрейме показал себя хуже (что парадоксально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenized = features.apply( #sample['text'].apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True, max_length = tokenizer.model_max_length,truncation=True ))\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159500, 512)\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#model = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "model = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "#model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06dd58b4889a4da184523bde5c157de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1595 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.cuda.IntTensor(padded[batch_size*i:batch_size*(i+1)], device = device) \n",
    "        attention_mask_batch = torch.cuda.IntTensor(attention_mask[batch_size*i:batch_size*(i+1)], device = device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.to(device)\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append (torch.Tensor.cpu(batch_embeddings[0][:,0,:]).numpy())#(batch_embeddings[0][:,0,:]).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим количество получившихся эмбеддингов для тренировочной выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159500, 768)\n"
     ]
    }
   ],
   "source": [
    "#embed = torch.Tensor.cpu(embeddings)\n",
    "print(np.concatenate(embeddings).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "#features_test = np.concatenate(embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем таргет и получившиеся эмбеддинги на тестовую и тренировочную выборку в соотношении 20/80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, \n",
    "                                                                            target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим корректность сплита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31900, 768)\n"
     ]
    }
   ],
   "source": [
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве моделей используем Логистическую регрессию, LGBMClassifier, CatBoostClassifier, RandomForest Classifier, RidgeClassifier, VotingClassifier, SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6630397036694062\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression(class_weight = 'balanced', random_state = 12345)\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "preds_train = model.predict(features_train)\n",
    "score = f1_score(predictions, target_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7047259997307123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "model_lgbm = LGBMClassifier(class_weight = 'balanced', random_state=42, num_iterations = 300)# n_estimators=80, max_depth = 9, num_iterations = 500)\n",
    "model_lgbm.fit(features_train, target_train)\n",
    "predicted_test = model_lgbm.predict(features_test)\n",
    "score_lgbm = f1_score(predicted_test, target_test)\n",
    "print(score_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.081683\n",
      "0:\tlearn: 0.7884850\ttotal: 720ms\tremaining: 11m 59s\n",
      "100:\tlearn: 0.8907708\ttotal: 16.1s\tremaining: 2m 23s\n",
      "200:\tlearn: 0.9100554\ttotal: 30.5s\tremaining: 2m 1s\n",
      "300:\tlearn: 0.9252283\ttotal: 44.9s\tremaining: 1m 44s\n",
      "400:\tlearn: 0.9392663\ttotal: 59s\tremaining: 1m 28s\n",
      "500:\tlearn: 0.9506036\ttotal: 1m 12s\tremaining: 1m 12s\n",
      "600:\tlearn: 0.9603182\ttotal: 1m 27s\tremaining: 58.1s\n",
      "700:\tlearn: 0.9674765\ttotal: 1m 41s\tremaining: 43.4s\n",
      "800:\tlearn: 0.9727545\ttotal: 1m 55s\tremaining: 28.7s\n",
      "900:\tlearn: 0.9765909\ttotal: 2m 9s\tremaining: 14.2s\n",
      "999:\tlearn: 0.9794623\ttotal: 2m 23s\tremaining: 0us\n",
      "0.7058980874682359\n"
     ]
    }
   ],
   "source": [
    "model_cb = CatBoostClassifier( random_state=42, eval_metric = 'F1', \n",
    "                              iterations = 1000, auto_class_weights = 'Balanced')# , depth=9)\n",
    "model_cb.fit(features_train, target_train, verbose=100)\n",
    "predicted_cb = model_cb.predict(features_test)\n",
    "score_cb = f1_score(predicted_cb, target_test)\n",
    "print(score_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5286481647269472\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_forest = RandomForestClassifier(random_state=42, n_estimators=120, max_depth = 8)\n",
    "model_forest.fit(features_train, target_train)\n",
    "predicted_forest = model_forest.predict(features_test)\n",
    "score_cb = f1_score(predicted_forest, target_test)\n",
    "print(score_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64889997731912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "model_rdg = RidgeClassifier(class_weight = 'balanced')\n",
    "model_rdg.fit(features_train, target_train)\n",
    "predictions = model_rdg.predict(features_test)\n",
    "score = f1_score(predictions, target_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6618257261410788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m1 = LGBMClassifier(class_weight = 'balanced', random_state=42, num_iterations = 400)\n",
    "m2 = SGDClassifier(class_weight = 'balanced', random_state = 12345)\n",
    "m3 = LogisticRegression(class_weight = 'balanced')\n",
    "m4 = RandomForestClassifier(random_state=42, n_estimators=80, max_depth = 9)\n",
    "model_vtng = VotingClassifier(estimators=[('lgbm', m1), ('sgdc', m2), ('regr', m3)], voting='hard')\n",
    "model_vtng.fit(features_train, target_train)\n",
    "predictions = model_vtng.predict(features_test)\n",
    "score = f1_score(predictions, target_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751412429378531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_sgd = SGDClassifier(class_weight = 'balanced')#, random_state = 42)\n",
    "model_sgd.fit(features_train, target_train)\n",
    "predictions = model_sgd.predict(features_test)\n",
    "score = f1_score(predictions, target_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, BERT на данном датасете показала себя достаточно плохо. Были предприняты попытки взятия разных семплов (2000, 10000, 20000, полный датасет, те записи, где длина меньше 512 символов), использование разных предтренированных моделей, но в среднем F1 на данных настройках при применении любых моделей не превышает 0.7. При обработке полной выборки лучше всего показал себя SGDClassifier c результатом 0.75 (надо заметить, не без рандома)."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 450,
    "start_time": "2021-07-27T12:44:32.013Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-27T12:44:36.105Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
